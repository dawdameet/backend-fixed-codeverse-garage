[
  {
    "id": 1,
    "title": "Incorrect VADER Sentiment Threshold",
    "description": "The sentiment analysis is heavily skewed towards 'neutral'. Even clearly positive headlines (e.g., 'stock surges') are classified as neutral. This results in an inaccurate VADER distribution and a 'neutral' overall sentiment, even when the news is good.",
    "expected": "Positive headlines should be classified as 'positive' with appropriate VADER compound scores. The threshold for positive sentiment should correctly identify bullish news.",
    "current": "Headlines with positive sentiment are being classified as neutral. The VADER distribution shows an overwhelming majority of 'neutral' classifications even for clearly positive/negative news.",
    "files": "market_sentiment/sentiment_analyzer.py",
    "difficulty": "easy",
    "points": 10,
    "steps": "1. Run the sentiment analyzer on sample headlines\n2. Observe that positive headlines are classified as neutral\n3. Check the VADER compound score threshold in analyze_text_vader method\n4. Identify that the positive threshold is set incorrectly\n5. Adjust the threshold to match VADER's standard thresholds",
    "hints": "Look at in the analyze_text_vader method",
    "solution": "Change 'if compound >= 0.5:' to 'if compound >= 0.05:'"
  },
  {
    "id": 2,
    "title": "Reversed Stopword Filter Logic",
    "description": "The word cloud and 'Top 10 Keywords' list show only common English stop words like 'and', 'the', 'for', 'is', etc., instead of relevant financial terms. The keyword extraction is completely broken.",
    "expected": "Keywords should be meaningful financial and domain-specific terms like 'stock', 'earnings', 'revenue', 'market', etc. Stop words should be filtered out.",
    "current": "The keyword list and word cloud contain only stop words (and, the, for, is, of, to). All meaningful words are being removed.",
    "files": "market_sentiment/sentiment_analyzer.py",
    "difficulty": "medium",
    "points": 20,
    "steps": "1. Run the sentiment analyzer and examine the word cloud\n2. Notice that only stop words appear in the word cloud\n3. Check the Top 10 Keywords output\n4. Examine the extract_keywords method\n5. Look at the list comprehension that filters words\n6. Identify that the filter logic is reversed",
    "hints": "The 'in' operator needs to become 'not in'",
    "solution": "Change 'all_words.extend([w for w in words if w in stop_words and len(w) > 3])' to 'all_words.extend([w for w in words if w not in stop_words and len(w) > 3])'"
  },
  {
    "id": 3,
    "title": "Overly Aggressive Regex Pattern",
    "description": "The word cloud shows malformed, jumbled 'words' that are actually multiple words concatenated together (e.g., 'teslastocksurgesonrecord...'). The Top 10 Keywords list is empty or shows these same jumbled strings.",
    "expected": "Words should be properly separated and individual. The regex should clean special characters while preserving word boundaries.",
    "current": "All spaces are being removed from text, causing all words in a headline to be mashed together into one giant string.",
    "files": "market_sentiment/sentiment_analyzer.py",
    "difficulty": "hard",
    "points": 30,
    "steps": "1. Run the analyzer and observe jumbled words in the word cloud\n2. Examine the clean_text method\n3. Look at the regex pattern being used\n4. Identify that the pattern removes ALL non-letter characters\n5. Realize that spaces are non-letter characters and are being removed\n6. Modify the regex to preserve whitespace",
    "hints": "The pattern should be r'[^A-Za-z\\s]' to keep letters AND spaces",
    "solution": "Change 'text = re.sub(r'[^A-Za-z]', '', text)' to 'text = re.sub(r'[^A-Za-z\\\\s]', '', text)'"
  },
  {
    "id": 4,
    "title": "Headlines List Overwrite",
    "description": "The Yahoo News analysis shows every single headline as identical - just N copies of the last headline fetched. The entire news DataFrame and subsequent analysis is worthless because it's analyzing one headline 50 times.",
    "expected": "Each row in the news DataFrame should contain a different headline from the Yahoo Finance API.",
    "current": "All headlines in the DataFrame are identical, showing only the last headline fetched from the API repeated multiple times.",
    "files": "market_sentiment/sentiment_analyzer.py",
    "difficulty": "extreme",
    "points": 40,
    "steps": "1. Run the analyzer and examine the news DataFrame output\n2. Notice all headlines are identical\n3. Examine the fetch_yahoo_news method carefully\n4. Find where headlines list is built in the loop\n5. Look for any code after the loop that modifies the headlines list\n6. Identify the line that overwrites the entire list\n7. Remove the problematic line",
    "hints": "This line should be completely removed, not modified",
    "solution": "Change 'headlines = [title] * len(headlines)' to '# Remove this line entirely'"
  },
  {
    "id": 5,
    "title": "Incorrect VaR Percentile Calculation",
    "description": "The Historical Value at Risk (VaR) calculation is inverted. Instead of showing the maximum potential loss (5th percentile), it shows the maximum gain (95th percentile). The VaR plot shows the red line on the positive side of the returns histogram instead of the negative side.",
    "expected": "VaR should represent the worst-case loss at a given confidence level. For 95% confidence, it should be the 5th percentile of returns (a negative value).",
    "current": "VaR is calculated as the 95th percentile instead of the 5th percentile, showing potential gains instead of losses.",
    "files": "portfolio/optimizer.py",
    "difficulty": "easy",
    "points": 10,
    "steps": "1. Run the portfolio optimizer\n2. Observe the VaR plot showing the red line on the positive side\n3. Examine the calculate_var method\n4. Check how the percentile is calculated\n5. Identify that it's using the confidence level directly instead of (1 - confidence_level)\n6. Fix the percentile calculation",
    "hints": "The current code uses confidence_level * 100 = 95th percentile",
    "solution": "Change 'var_historical = np.percentile(portfolio_returns, confidence_level*100)' to 'var_historical = np.percentile(portfolio_returns, (1-confidence_level)*100)'"
  },
  {
    "id": 6,
    "title": "Missing Volatility Annualization",
    "description": "All portfolio metrics are nonsensical. Annual Volatility is extremely small (e.g., 1.2% when it should be ~19%), and Sharpe Ratio is absurdly large (e.g., 15.0 instead of ~0.8). Returns are annualized but variance is not.",
    "expected": "Both returns and variance should be annualized using the same scaling factor (252 trading days). Volatility = sqrt(daily_variance * 252).",
    "current": "Returns are multiplied by 252 but the covariance matrix is not, causing a scaling mismatch.",
    "files": "portfolio/optimizer.py",
    "difficulty": "medium",
    "points": 20,
    "steps": "1. Run the optimizer and observe unrealistic volatility values\n2. Notice the Sharpe ratio is extremely high\n3. Examine the calculate_portfolio_metrics method\n4. Check how annual returns are calculated (multiplied by 252)\n5. Check how portfolio variance is calculated\n6. Identify that the covariance matrix is not being annualized\n7. Add the 252 scaling factor to the covariance",
    "hints": "Multiply self.returns.cov() by 252",
    "solution": "Change 'np.dot(weights.T, np.dot(self.returns.cov(), weights))' to 'np.dot(weights.T, np.dot(self.returns.cov() * 252, weights))'"
  },
  {
    "id": 7,
    "title": "Invalid Portfolio Weight Normalization",
    "description": "The Efficient Frontier plot is completely broken. Instead of a smooth 'C' shaped curve, it appears as a small, scattered cloud of points. The Monte Carlo simulation generates invalid portfolios.",
    "expected": "Portfolio weights should sum to 1.0 for each simulated portfolio. The normalization should divide by the sum of the weights.",
    "current": "Weights are divided by the number of assets instead of the sum of weights, creating invalid portfolios that don't sum to 1.",
    "files": "portfolio/optimizer.py",
    "difficulty": "hard",
    "points": 30,
    "steps": "1. Run the optimizer and observe the malformed Efficient Frontier plot\n2. Examine the generate_efficient_frontier method\n3. Look at how random weights are generated and normalized\n4. Check what weights are divided by\n5. Realize that dividing by n_assets doesn't make weights sum to 1\n6. Fix to divide by the actual sum of the weights",
    "hints": "Use np.sum(weights) as the denominator",
    "solution": "Change 'weights /= n_assets' to 'weights /= np.sum(weights)'"
  },
  {
    "id": 8,
    "title": "Incorrect Beta Calculation Formula",
    "description": "The ML Asset Clustering feature uses flawed beta values. Beta is calculated by dividing covariance by the asset's own variance instead of the market's variance. This produces nonsensical beta values, causing K-Means to group assets incorrectly.",
    "expected": "Beta = Cov(asset, market) / Var(market). It measures how much the asset moves relative to the market.",
    "current": "Beta = Cov(asset, market) / Var(asset). This is mathematically incorrect and produces meaningless clustering.",
    "files": "portfolio/optimizer.py",
    "difficulty": "extreme",
    "points": 40,
    "steps": "1. Run the optimizer and examine the cluster assignments\n2. Look at the beta values in the features DataFrame\n3. Notice beta values don't match financial intuition\n4. Examine the ml_asset_clustering method\n5. Check the beta calculation formula in the lambda function\n6. Identify that it divides by x.var() instead of market_proxy.var()\n7. Correct the formula",
    "hints": "The denominator should be market_proxy.var()",
    "solution": "Change 'features['beta'] = self.returns.apply(lambda x: x.cov(market_proxy) / x.var())' to 'features['beta'] = self.returns.apply(lambda x: x.cov(market_proxy) / market_proxy.var())'"
  },
  {
    "id": 9,
    "title": "Identical Plot Lines for Actual vs Predicted",
    "description": "The prediction plots are useless because the 'Actual' and 'Predicted' lines are perfectly identical. This makes it impossible to visually evaluate model accuracy.",
    "expected": "The plot should show two different lines: one for actual prices and one for predicted prices, allowing visual comparison.",
    "current": "Both lines are identical because the same data (predicted) is being plotted twice.",
    "files": "stock_price_predictor/predictor.py",
    "difficulty": "easy",
    "points": 10,
    "steps": "1. Run the stock predictor\n2. Observe that Actual and Predicted lines in plots overlap perfectly\n3. Examine the plot_predictions method\n4. Look at the two ax.plot() calls\n5. Notice that both are plotting 'predicted'\n6. Change the first one to plot 'actual'",
    "hints": "The first plot command should use 'actual'",
    "solution": "Change 'ax.plot(predicted, label='Actual', linewidth=2, color='blue')' to 'ax.plot(actual, label='Actual', linewidth=2, color='blue')'"
  },
  {
    "id": 10,
    "title": "Data Leakage from Shuffling Time Series",
    "description": "Linear Regression and Random Forest models report unrealistically high R² scores (0.999 or 1.000). The models appear 'perfect', which is impossible for stock prediction.",
    "expected": "Time series data should be split chronologically without shuffling. Past data trains the model, future data tests it.",
    "current": "train_test_split is shuffling past and future data together, causing massive data leakage. The model sees future prices during training.",
    "files": "stock_price_predictor/predictor.py",
    "difficulty": "medium",
    "points": 20,
    "steps": "1. Run the predictor and observe unrealistically high R² scores\n2. Examine the prepare_ml_data method\n3. Look at the train_test_split call\n4. Notice there's no shuffle parameter specified\n5. Research that train_test_split shuffles by default\n6. Add shuffle=False parameter",
    "hints": "Add shuffle=False to preserve chronological order",
    "solution": "Change 'return train_test_split(X, y, test_size=0.2)' to 'return train_test_split(X, y, test_size=0.2, shuffle=False)'"
  },
  {
    "id": 11,
    "title": "Incorrect RSI Calculation Formula",
    "description": "The RSI (Relative Strength Index) feature is computed as NaN or nonsensical negative values. This corrupts the feature data for all ML models, leading to poor predictions.",
    "expected": "RSI should be a value between 0 and 100, calculated from average gains and average losses over a window period.",
    "current": "The loss component is calculated incorrectly, producing negative values where positive values are needed, breaking the RSI formula.",
    "files": "stock_price_predictor/predictor.py",
    "difficulty": "hard",
    "points": 30,
    "steps": "1. Run the predictor and examine the features DataFrame\n2. Notice RSI values are NaN or negative\n3. Examine the create_features method\n4. Look at the RSI calculation\n5. Understand that RSI needs average gain and average loss\n6. Check the 'loss' calculation using .where()\n7. Identify that loss values are negative when they should be positive\n8. Negate the loss expression",
    "hints": "Negate the expression: -delta.where(delta < 0, 0)",
    "solution": "Change 'loss = (delta.where(delta < 0, 0)).rolling(window=14).mean()' to 'loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()'"
  },
  {
    "id": 12,
    "title": "Fake Future Prediction using Naive Formula",
    "description": "The predict_future function claims to use the 'best model' (LSTM) but actually generates predictions using a naive exponential formula based on historical average returns. The predictions are fake.",
    "expected": "Future predictions should use the best-performing ML model's .predict() method with properly prepared input sequences.",
    "current": "The code identifies the best model but never uses it. Instead, it calculates future prices with a simple formula: last_price * (1 + avg_change)^i",
    "files": "stock_price_predictor/predictor.py",
    "difficulty": "extreme",
    "points": 40,
    "steps": "1. Run the predictor and observe the future predictions\n2. Notice the predictions form a very smooth exponential curve\n3. Examine the predict_future method\n4. See that best_model_name is correctly identified\n5. Look for where self.models[best_model_name].predict() is called\n6. Notice it's never called\n7. Find the line calculating future_prices with a formula\n8. Realize this line must be replaced with actual model prediction logic",
    "hints": "This requires significant refactoring, not just a one-line fix",
    "solution": "Change 'future_prices = [last_price * (1 + avg_change) ** i for i in range(1, days+1)]' to '# This requires implementing proper iterative prediction:\n# 1. Get the last sequence_length days of features\n# 2. Use best_model.predict() to get next day\n# 3. Update the sequence with the prediction\n# 4. Repeat for 'days' iterations'"
  },
  {
    "id": 13,
    "title": "Inverted Log Return Calculation",
    "description": "All calculated returns have inverted signs. Positive returns appear negative and vice versa. The jump_mu parameter shows as negative when it should be positive, and simulated prices drift downward instead of upward.",
    "expected": "Log return should be ln(P_t / P_{t-1}), giving positive values when price increases.",
    "current": "Returns are calculated as ln(P_{t-1} / P_t), giving negative values when price increases.",
    "files": "cpp_services/monte_carlo/monte_carlo.cpp",
    "difficulty": "medium",
    "points": 20,
    "steps": "1. Run the Monte Carlo simulation\n2. Observe that simulated prices trend downward\n3. Check the jump_mu and drift parameters\n4. Notice they have opposite signs from expected\n5. Examine the log_returns function\n6. Identify that the division order in log() is reversed\n7. Swap the numerator and denominator",
    "hints": "The indices in the division are backwards",
    "solution": "Change 'double ret = log(data[i-1].close / data[i].close);' to 'double ret = log(data[i].close / data[i-1].close);'"
  },
  {
    "id": 14,
    "title": "Incorrect EWMA Weighting Formula",
    "description": "The Exponentially Weighted Moving Average (EWMA) is overly sensitive to noise. The drift parameter is inaccurate, and the historical mean doesn't smooth properly.",
    "expected": "EWMA should give more weight to recent values. Formula: s_t = α * x_t + (1-α) * s_{t-1}",
    "current": "The weights are swapped: s_t = (1-α) * x_t + α * s_{t-1}, giving more weight to old values.",
    "files": "cpp_services/monte_carlo/monte_carlo.cpp",
    "difficulty": "medium",
    "points": 20,
    "steps": "1. Observe that EWMA values don't properly smooth the time series\n2. Examine the ewma function\n3. Look at the weighting formula\n4. Recall that alpha is the weight for the current value\n5. Identify that current code gives weight (1-alpha) to current value\n6. Swap the weights",
    "hints": "The formula has these weights reversed",
    "solution": "Change 's = (1.0 - alpha) * x[i] + alpha * s;' to 's = alpha * x[i] + (1.0 - alpha) * s;'"
  },
  {
    "id": 15,
    "title": "Independent Random Variables for Correlated Components",
    "description": "The GARCH volatility model shows excessive uncorrelated volatility. Variance spread is too wide, and price paths have too much random noise. The model violates financial assumptions.",
    "expected": "The GARCH volatility shock and the price diffusion term should use the SAME random variable to be correlated per the model specification.",
    "current": "Two different random numbers are generated, making the volatility shock and price movement independent.",
    "files": "cpp_services/monte_carlo/monte_carlo.cpp",
    "difficulty": "hard",
    "points": 30,
    "steps": "1. Observe excessive variance in simulated price paths\n2. Examine the main simulation loop\n3. Look at where random variables are generated\n4. Notice two different random draws: one for GARCH, one for diffusion\n5. Understand that they should share the same random shock\n6. Modify diffusion to reuse the same 'z' variable",
    "hints": "Look for where 'z' is first generated and reuse it",
    "solution": "Change 'double diffusion = sigma * sqrt(dt) * nd(gen);' to 'double diffusion = sigma * sqrt(dt) * z;'"
  },
  {
    "id": 16,
    "title": "Mis-mapped Percentile Output Variables",
    "description": "Best-case and worst-case scenarios are swapped in the JSON output. The p5 percentile shows high values, and p95 shows low values. Labels don't match actual percentile values.",
    "expected": "p5_final should contain the 5th percentile value, p95_final should contain the 95th percentile value.",
    "current": "The variable assignments are crossed: p5 value is written to p95_final key and vice versa.",
    "files": "cpp_services/monte_carlo/monte_carlo.cpp",
    "difficulty": "easy",
    "points": 10,
    "steps": "1. Run the Monte Carlo simulation\n2. Examine the JSON output\n3. Notice p5_final has a higher value than p95_final\n4. Check the output generation code\n5. Find where p5 and p95 are written to JSON\n6. Identify that the variable names are swapped\n7. Correct the mapping",
    "hints": "The p5 variable is being written to p95_final and vice versa",
    "solution": "Change '\"p5_final\": \" << p95 ... \"p95_final\": \" << p5' to '\"p5_final\": \" << p5 ... \"p995_final\": \" << p95'"
  },
  {
    "id": 17,
    "title": "Inverted PnL Calculation for Short Trades",
    "description": "The profit and loss (PnL) for short trades is calculated incorrectly, resulting in inverted gains/losses for short positions. Profitable short trades appear as losses and vice versa.",
    "expected": "Short trade PnL should be (entry - exit) / entry, since you profit when price falls after shorting.",
    "current": "The code uses the long trade formula (exit - entry) / entry for short trades, inverting the signs.",
    "files": "cpp_services/pair_trading/backtest.cpp",
    "difficulty": "easy",
    "points": 10,
    "steps": "1. Run the pair trading backtest\n2. Observe that short trades show incorrect PnL signs\n3. Examine the trade exit logic\n4. Find the PnL calculation for short positions\n5. Identify that it uses the long formula\n6. Invert the formula for short trades",
    "hints": "Look for where position == -1 (short)",
    "solution": "Change 'double tradePnl = (exitPrice - entryPrice) / entryPrice;' to 'double tradePnl = (entryPrice - exitPrice) / entryPrice;'"
  },
  {
    "id": 18,
    "title": "Flipped Mean-Reversion Entry Logic",
    "description": "The trading strategy implements the opposite of mean-reversion. It shorts when the spread is low (should long) and longs when the spread is high (should short). The strategy loses money systematically.",
    "expected": "Mean-reversion: go LONG when spread < lower bound (spread will revert up), go SHORT when spread > upper bound (spread will revert down).",
    "current": "The position assignments are backwards: SHORT when spread < lower bound, LONG when spread > upper bound.",
    "files": "cpp_services/pair_trading/backtest.cpp",
    "difficulty": "medium",
    "points": 20,
    "steps": "1. Run the backtest and observe poor/negative returns\n2. Understand mean-reversion strategy logic\n3. Examine the entry signal conditions\n4. Check what position is assigned when spread < lowerBound\n5. Check what position is assigned when spread > upperBound\n6. Identify that these are backwards\n7. Swap the position assignments",
    "hints": "Swap the 1 and -1 assignments",
    "solution": "Change 'if (spread < lowerBound) { position = -1; ... } else if (spread > upperBound) { position = 1; ... }' to 'if (spread < lowerBound) { position = 1; ... } else if (spread > upperBound) { position = -1; ... }'"
  },
  {
    "id": 19,
    "title": "Off-by-One Window Size Error",
    "description": "The moving average and standard deviation are calculated on a window of N-1 items instead of N items. This skews all trading signals because the statistics are based on incomplete windows.",
    "expected": "The window should contain exactly LOOKBACK_WINDOW items for statistical calculations.",
    "current": "Items are removed from the window too early, when size >= N instead of size > N, causing the window to max out at N-1 items.",
    "files": "cpp_services/pair_trading/backtest.cpp",
    "difficulty": "difficult",
    "points": 30,
    "steps": "1. Observe that trading signals seem slightly off\n2. Examine the window management code\n3. Look at the condition for removing old items\n4. Trace through: when does the first item get removed?\n5. Realize it's removed right after adding the Nth item\n6. Understand this keeps window size at N-1\n7. Change >= to >",
    "hints": "This is a classic off-by-one error",
    "solution": "Change 'if (spreadWindow.size() >= LOOKBACK_WINDOW) {' to 'if (spreadWindow.size() > LOOKBACK_WINDOW) {'"
  },
  {
    "id": 20,
    "title": "Lookahead Bias in Signal Calculation",
    "description": "The backtest is invalid because trading decisions for day 't' use data from day 't' itself. This is lookahead bias - using future information that wouldn't be available at decision time.",
    "expected": "Signals for day 't' should be calculated using only data up to day 't-1'. Then the decision is made, and day 't' data is added after.",
    "current": "Day 't' spread is added to the window BEFORE calculating signals, meaning the signal uses information from the current day.",
    "files": "cpp_services/pair_trading/backtest.cpp",
    "difficulty": "extreme",
    "points": 40,
    "steps": "1. Understand the temporal sequence of a backtest\n2. Examine the main simulation loop carefully\n3. Identify the order of operations: add to window, calculate signals, make decisions\n4. Realize that when signals are calculated, today's spread is already in the window\n5. Understand this means the decision uses future information\n6. Restructure the loop: calculate signals first, make decisions, THEN add today's data",
    "hints": "Move spreadWindow.push_back(spread) to after all trading logic",
    "solution": "Change '// Current order: push_back(spread), then calculateMeanAndStdDev, then decide' to '// Correct order: calculateMeanAndStdDev, decide, then push_back(spread)'"
  },
  {
    "id":32,
    "title": "for loop starts from 1",
    "description": "loop is starting from 1 and not 0",
    "expected": "loop must start from 0",
    "current": "loop starts from 0.",
    "files": "bug/main.py",
    "difficulty": "extreme",
    "points": 40,
    "steps": "i should start from 0 and not 1 in the range function",
    "hints": "make the range functions first argument as 0 instead of 1 ",
    "solution": "for i in range(0, 10):\n \tprint(i)"
  }
]